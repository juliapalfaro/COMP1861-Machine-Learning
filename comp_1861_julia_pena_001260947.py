# -*- coding: utf-8 -*-
"""COMP_1861_Julia_Pena_001260947.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12xDeBZjVNBmGP2A2BmJuDqshE_2ZBf3E

# Project Summary

This project focuses on predicting Airbnb listing prices in New York City using machine learning models. The objective is help hosts optimise their pricing while maintaining market competitiveness and considering key factors such as location, room type and availability.

Predicting Airbnb prices is crucial for both hosts and guests so this study aims to develop a data-driven approach to price prediction and identifying key factos influencing price variations.

To achieve, I selected and implemented two machine learning models: Linear Regression and Random Forest. Before implemented, the dataset was preprocessed by handling missing values and outliers, enconding categorical variables and standardizing numerical features. The performance of both models was evaluated using MSE, RMSE and MAE.

# Data Exploration

In this section, we analyze the Airbnb listings dataset for New York City to identify key pricing patterns, assess data quality issues, and determine the most relevant features for our machine learning model. The focus is on understanding data distribution, feature relationships, and potential data preprocessing needs.

Our analysis revealed several important factors that will guide data preprocessing. There are significant outliers in price, with some listings having extreme values that may require removal or transformation. Similarly, minimum_nights shows unrealistic values, where some listings require excessively long stays that do not reflect typical booking patterns. The calculated_host_listings_count feature also indicates that while most hosts manage only a few listings, some control dozens or even hundreds, suggesting the presence of professional property management companies rather than individual hosts.

Additionally, missing values in last_review (20%) and high-cardinality features such as name, host_id, host_name, neighbourhood, latitude, and longitude do not contribute directly to price prediction and will likely be dropped. When analyzing pricing trends, we found that room type and location are the strongest influencers. Entire homes and apartments tend to be priced significantly higher than private rooms, and listings in Manhattan are generally more expensive than those in other boroughs.

These insights will inform the next steps in data preprocessing, ensuring a refined dataset that improves model performance and reliability.

Before analysing the data, we first load the dataset and perform an initial inspection by displaying basic dataset information with **info()** to understand column names, data types, and missing values. To preview the first few rows we used **head()** and to generate a summary statistics for numerical feautures **describe ()**.

To assess feature diversity and potential categorical needs, we have analysed the number of unique values in categorical columns with **nunique()**, a high-cardinality features mean that may need grouping.
"""

#Understand the Data Structure

#Load and inspect the dataset

from google.colab import drive
drive.mount('/content/drive')
import pandas as pd

df=pd.read_csv('/content/drive/My Drive/Coursework/AB_NYC_2019.csv')

print(df.info())
print(df.head())
print(df.describe())

#Check unique values per column to understand feature diversity

print(df.nunique())

"""Later,to ensure data completness, first, we have checked for missing values in each column with **isnull().sum()**, second, we need to identify the percentage of missing data of the column which are showing missing values so it would help us to determine whether to drop or impute a column based on its missing data proportion. As we can see last_review and reviews_per_month have 20% missing values."""

#Handle Missing Values

#Check missing values
print(df.isnull().sum())

#Identify percentage of missing values
missing_values_percent=(df.isnull().sum()/len(df))*100
missing_values_percent=missing_values_percent[missing_values_percent>0].sort_values(ascending=False)
print(missing_values_percent)

"""The price, minimum_nights, and calculated_host_listings_count variables exhibit high dispersion, as indicated by their minimum and maximum values and large standard deviations. We used boxplots to further investigate potential outliers. The boxplots reveal that while minimum_nights and calculated_host_listings_count contain extreme outliers, price shows a more scattered distribution but still contains notable outliers. The anomalies in price may indicate extreme pricing strategies, minimum_nights outliers could suggest unrealistic stay durations, and calculated_host_listings_count outliers might highlight a few hosts dominating the market."""

#Data distribution and Outliers

import matplotlib.pyplot as plt
import seaborn as sns

# Boxplots to detect outliers
for col in ['price', 'minimum_nights', 'calculated_host_listings_count']:
    plt.figure(figsize=(8, 4))
    sns.boxplot(x=df[col])
    plt.title(f'Boxplot of {col}')
    plt.show()

"""I analyzed the relationships between numerical features by computing a correlation matrix and visualizing it with a heatmap for better interpretation. Based on the findings, there are no strong correlations between price and other numerical features. This suggests that price may be influenced more by categorical variables such as neighbourhood_group or room_type and external factors rather than numerical attributes."""

#Correlations and feature relationships

#Correlation in numerical features
print(df.corr(numeric_only=True))

#Correlation matrix
plt.figure(figsize=(10,6))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm', linewidths=0.5)
plt.title("Correlation Heatmap")
plt.show()

"""For categorical features, as **neighbourhood_group**, **neighbourhood** and **room_type**, I have checked the amount of unique values and to visualize their distributions I have used bar charts which conclusions are that **room_type** has a strong influcence on **price**, with entire home/apartments being the most expensive, **neighbourhood_group** shows price variation with Manhattan listings being the most expensive, and some categories such as shared room for **room_type**, and staten island and bronx for **neighbourhood_group** have low representation which may affect the model perfomance."""

#Categorical analysis

#To check unique values in categorical columns

for col in ['neighbourhood_group', 'neighbourhood', 'room_type']:
    print(df[col].value_counts())

#Visualize categorical data distributions

sns.countplot(x='neighbourhood_group', data=df)
plt.show()

sns.countplot(x='room_type', data=df)
plt.show()

"""Thereafter, we have explored how price correlates with other variables as **number_of_reviews**, **neighbourhood_group**, **room_type**, **calculated_host_listings_count** through a scatter plott."""

#Identify trends and patterns

#Investigate the relationship between price and other features

sns.scatterplot(x=df['price'], y=df['number_of_reviews'])
plt.show()

sns.scatterplot(x=df['price'], y=df['neighbourhood_group'])
plt.show()

sns.scatterplot(x=df['price'], y=df['room_type'])
plt.show()

sns.scatterplot(x=df['price'], y=df['calculated_host_listings_count'])
plt.show()

sns.scatterplot(x=df['price'], y=df['minimum_nights'])
plt.show()

"""Regarding data quality checks, first, checking the dataset for duplicates, no duplicates row were found in the dataset. Identifying inconsistencies, found some listings with a price of 0USD which are likely errors or higher than 10, 000USD which may requires capping. Unrealistic values as +365 for minimum nights for airbnb stays, which suggests entry errors or long-term rentals. Also other listings has an availability of 0 which mean they are not available for booking, we may consider removing them if they do not contribute to the price prediction. In addition, we have checked for inbalanced categories which may afect the modelling, looking into neirghbourhood_group and room_type, they are well balanced overall but maybe considering grouping shared room with private room due to its low representation. In neirghbourhood_group might consider removing staten island because it lacks of statiscal significance."""

#Data Quality Checks

#Check for duplicate rows

print(df.duplicated().sum())

#Identify inconsistencies

print(df[df['price'] <= 0])

print(df[df['price'] > 10000])

print(df[df['minimum_nights'] >= 365])

print(df[df['minimum_nights'] <= 0])

print(df[df['calculated_host_listings_count'] > 100])

print(df[df['availability_365'] <= 0])

#Looking for imbalanced categories

categorical_features = ['room_type', 'neighbourhood_group']

for feature in categorical_features:
    print(df[feature].value_counts(normalize=True) * 100)
    print("\n")

"""# Data Preprocessing

During this markdown, the dataset is going to be cleaned and transformed so it is ready for machine learning.
Following the conclusion in the data exploration phase the key actions are:
droping irrelevant columns, handle missing vales, group together low-representation categories, enconding catgegorial data, handle outliers, and feature scaling.
First, since some columns were not contributing to price prediction, I decide to remove them, they are: **id**, **name**, **host_id**, **last_review**, **latitude**, **longitude**, **neighbourhood**.
"""

#Dropping columns

print(df.info())

drop_columns = ['id','name', 'host_id', 'host_name', 'last_review', 'latitude', 'longitude', 'neighbourhood']

df.drop(columns=drop_columns, inplace=True)

print(df.info())#checking if the columns have beend dropped

"""For handle missing values in reviews_per_month with a 20% of missing values, they have been filled with 0, considering they do not have any reviews because they are new."""

#Filling missing values

df['reviews_per_month'].fillna(0, inplace=True)

print(df.isnull().sum())#checking that reviews_per_month does not have any missing values

"""In **neighbourhood_group** and **room_type**, we observed that some categories as bronx or staten island for **neighbourhood_group** and shared room for **room_type**. So in **neighbourhood_group** I decide to create a new category called 'Other' and merged bronx and staten island listings. For **room_type**, merging private and shared room into a new category called Private/Shared room seemed the most reasonable."""

print(df['room_type'].unique())#Verifying unique values before merging

df['room_type'] = df['room_type'].str.strip().str.lower() #standardize room type formatting to void errors


df['room_type'] = df['room_type'].replace({'shared room': 'shared/private room',
                                           'private room': 'shared/private room'}) #merge shared room and private room

print(df['room_type'].unique())#new categories
print(df['room_type'].value_counts())#new categories merged

print(df['neighbourhood_group'].unique())#verifying unique values before merging

df['neighbourhood_group'] = df['neighbourhood_group'].str.strip().str.lower()#standardize neighbourhood_group to void errors


df['neighbourhood_group'] = df['neighbourhood_group'].replace({'staten island': 'other',
                                                               'bronx':'other'})#merging into a new category

print(df['neighbourhood_group'].unique())#new categories
print(df['neighbourhood_group'].value_counts())#new categories merged

"""In addition, as they were the only two categorical features remaining, I applied one-hot encoding that converted them into numerical format as 0 or 1, which means that when it is 1 in Manhattan, the listing is located in Manhatan and the same applies to room_type. This method create a column for each value like **neighbourhood_group_manhattan**, **neighbourhood_group_queens**, etc. and after applied the 0/1 combination to know which is in the listing."""

df_encoded = pd.get_dummies(df, columns=['neighbourhood_group', 'room_type'])#converting text to numerical

print(df_encoded.head())

#Convert True/False to 0/1
df_encoded ['neighbourhood_group_brooklyn'] = df_encoded ['neighbourhood_group_brooklyn'].astype(int)
df_encoded ['neighbourhood_group_manhattan'] = df_encoded ['neighbourhood_group_manhattan'].astype(int)
df_encoded ['neighbourhood_group_queens'] = df_encoded ['neighbourhood_group_queens'].astype(int)
df_encoded ['neighbourhood_group_other'] = df_encoded ['neighbourhood_group_other'].astype(int)

df_encoded ['room_type_entire home/apt'] = df_encoded ['room_type_entire home/apt'].astype(int)
df_encoded ['room_type_shared/private room'] = df_encoded ['room_type_shared/private room'].astype(int)

print(df_encoded.head())

"""To analyze the relationship between encoded categorical variables (room_type and neighbourhood_group) and price, we computed a new correlation matrix. The results show that the strongest correlations with price are observed in:
  neighbourhood_manhattan:indicating that listings in Manhattan tend to be more expensive.
  entire home/apt:suggesting that renting an entire property is generally cost more than shared accommodations.
These findings align with expectations, as location and property type significantly influence pricing.
"""

#Correlations and feature relationships

#Correlation in numerical features
print(df_encoded.corr(numeric_only=True))

#Correlation matrix
plt.figure(figsize=(10,6))
sns.heatmap(df_encoded.corr(numeric_only=True), annot=True, cmap='coolwarm', linewidths=0.5)
plt.title("Correlation Heatmap")
plt.show()

"""In the data exploration, we found outliers in **price**, **minmum_nights** and **calculated_host_listings** and I have managed them in different ways. For **minimum_nights**, I have removed any listing with more than 180 days as minimum nights, for price I have decided to keep the outlier considering is a dataset for airbnb stays in New York where you can find really exclusive areas and listings with really high prices. So applying log trasnformation reduces the impact of extreme outliers and make the price distribution more normal which is better for model perfomance that later can be converted back. Following the log transformation, we decide to scale numerical columns to ensures that they have equal importance in the model.
Also, we have removed any lisitings with an **availability_365** equal to 0 which means they cannot be booked.
"""

#Removing listing with minimum nights more than 180

df_encoded = df_encoded[df_encoded['minimum_nights'] <= 180]

#Log scaling price

import numpy as np

df_encoded['price_log'] = np.log1p(df_encoded['price'])

sns.histplot(df_encoded['price_log'], bins=30, kde=True)
plt.title("Log transformed prices")
plt.show()

#Removing listings with no availability

df_encoded = df_encoded[df_encoded['availability_365'] > 0]

print(f"Remain listings: {df_encoded.shape[0]}")#checking how many listings have been removed

#Standardization

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df_encoded[['minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']] = \
    scaler.fit_transform(df_encoded[['minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']])

print(df_encoded.info())
print(df_encoded.head())#checking that the values have been correctly standardized

"""The last step is to saved the new dataset with a new name: 'AB_NYC_2019_Cleaned'"""

#Saving the cleaned data into a new CSV file

from google.colab import drive

df_encoded.to_csv('/content/drive/My Drive/Coursework/AB_NYC_2019_Cleaned.csv')

"""# Model Selection and Implementation

To predict Airbnb prices, I need to use supervised learning algorithms for regression for the target variable **price_log**. The two models selected are Linear Regression and Random Forest Regression.

Linear Regression computes the linear relationship between the dependent variable and one or more independant features which makes it suitable to predict airbnb price.

Random Forest Regressor builds multiple decisison trees and averages their prediction. Unlike Linear Regression can capture non-linear relationships between price and other features.

The motivation to compare this two models laid in that each model has advantages and by comparing them, we can determine the most suitable approach. While linear regression assumes a linear relationship, random forest caputures non-linear connections with variables. Linear regression stands out for interpretability and random forest is more powerful but less transparent. In this dataset we are handling outliers and random forest is more robust to extreme prices where linear regression is not. In conclusion, I have trained a linear regression algorithm as a benchmark model and random forest to confirm if the dataset have complex relationships.

For both models I have followed the same implementation steps:

1. Define x as feautures and y as target variable
2. Split the dataset into training and test sets (80% and 20%)
3. Train the model
4. Making some predictions with log prices and original prices
5. Storing prediction for performance evaluation
"""

y = df_encoded['price_log']#defining the target variable
x = df_encoded.drop(columns=['price', 'price_log'])#defining features less target variable

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)#spliting into training (80%) and test (20%)

print(f"Train set size: {x_train.shape[0]}")
print(f"Test set size: {x_test.shape[0]}")

#Model 1: Regression

from sklearn.linear_model import LinearRegression

linear_model = LinearRegression()
linear_model.fit(x_train, y_train)

print("Model is trained")#checking if the model has been trained correctly

#making some predictions

y_pred_linear = linear_model.predict(x_test)

print(y_pred_linear[:5]) #log prices

import numpy as np

y_pred_original = np.expm1(y_pred_linear)

print(y_pred_original[:5]) #original prices

#Model 2: Random Forest Regression

from sklearn.ensemble import RandomForestRegressor

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(x_train, y_train)

print("Model is trained")#checking if the model has been trained correctly

#making some predictions

y_pred_rf = rf_model.predict(x_test)

print(y_pred_rf[:5]) #log prices

y_pred_original_rf = np.expm1(y_pred_rf)

print(y_pred_original_rf[:5]) #original prices

#Model 3:XGBoost (Extreme Gradient Boosting)
import xgboost as xgb
import numpy as np
from sklearn.model_selection import train_test_split

# Initialize XGBoost Regressor
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=6)

# Train the model
xgb_model.fit(x_train, y_train)

print("XGBoost Model is trained")  # Checking if the model has been trained correctly

# Making predictions
y_pred_xgb = xgb_model.predict(x_test)

print(y_pred_xgb[:5])  # Log prices (predicted)

# Convert log prices back to original prices
y_pred_original_xgb = np.expm1(y_pred_xgb)

print(y_pred_original_xgb[:5])  # Original prices

"""# Evaluation Metrics

To evaluate the performance of both models, we have used Mean Squared Error which indicate better accuracy, Root Mean Squared Error that indicate better predictions and Mean Absolute Error that represents better model performance.

Using this metrics we have compared both models to determine which model provides better accuracy, lower error and stronger predictive perfomance for airbnb price estimation.
"""

#Model 1

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

# Evaluate Linear Regression model
mse_lr = mean_squared_error(y_test, y_pred_linear)
rmse_lr = np.sqrt(mse_lr)
mae_lr = mean_absolute_error(y_test, y_pred_linear)

# Print results
print("Linear Regression Performance:")
print(f"MSE: {mse_lr:.4f}")
print(f"RMSE: {rmse_lr:.4f}")
print(f"MAE: {mae_lr:.4f}")

#Model 2

# Evaluate Random Forest model
mse_rf = mean_squared_error(y_test, y_pred_rf)
rmse_rf = np.sqrt(mse_rf)
mae_rf = mean_absolute_error(y_test, y_pred_rf)


# Print results
print("Random Forest Performance:")
print(f"MSE: {mse_rf:.4f}")
print(f"RMSE: {rmse_rf:.4f}")
print(f"MAE: {mae_rf:.4f}")

#Model 3

# Evaluate model performance
mse_xgb = mean_squared_error(y_test, y_pred_xgb)
rmse_xgb = mse_xgb ** 0.5
mae_xgb = mean_absolute_error(y_test, y_pred_rf)

#Print results
print(f"XGBoost RMSE: {rmse_xgb}")
print(f"XGBoost MSE: {mse_xgb}")
print(f"XGBoost MAE: {mae_xgb}")

"""How to compare the models

MSE --> lower is better
RMSE --> lower is better
MAE --> lower is better

# Model Comparison and Recommendation

Random Forest has a lower MSE, RMSE, and MAE compared to Linear Regression, indicating that it provides more accurate price predictions. The smaller errors suggest that Random Forest captures more complex relationships in the data, improving price estimation.

Linear Regression, while simpler and more interpretable, assumes a linear relationship that may not fully explain the variations in Airbnb prices. It struggles with outliers and non-linear interactions, which reduces its predictive accuracy. On the other hand, Random Forest, being a tree-based model, is more robust to outliers and better suited for capturing non-linear relationships, making it a stronger candidate for this dataset. However, Random Forest takes longer to train, as it builds multiple decision trees to improve accuracy.

To further optimize the prediction model, we tested XGBoost, a gradient boosting algorithm that is often faster and more efficient than Random Forest. The evaluation results for XGBoost are as follows:

XGBoost RMSE: 0.489
XGBoost MSE: 0.239
XGBoost MAE: 0.362

These results indicate that XGBoost outperforms both Random Forest and Linear Regression, achieving the lowest error values. XGBoost's ability to capture complex patterns and interactions between features while remaining computationally efficient makes it the best-performing model among the three. Unlike Random Forest, which builds independent trees, XGBoost improves predictions by iteratively learning from previous errors, leading to a more refined model.

# OpenAI Comparison and Improvement

Your notebook primarily focuses on data exploration, missing values handling, outlier detection, correlations, and categorical data analysis using Pandas, Matplotlib, and Seaborn.

I'll generate an AI-optimized version of your code, ensuring:

Efficiency – Reduce redundancy and improve performance.
Readability – Improve clarity with structured functions and concise comments.
Additional Insights – Introduce any useful enhancements.
Let me generate an improved version of your code now. ​​

Comparison and Improvements
Here’s how the AI-generated code enhances your original implementation:

Modularization & Readability

The AI-generated code groups related tasks into functions, such as summarize_data(), check_missing_values(), and detect_outliers(), making it more readable and maintainable.
Your original code was written in sequential blocks, making it harder to reuse code efficiently.
Improved Missing Value Analysis

Your original approach printed missing values and their percentages separately.
The AI-generated version refines this process by sorting and displaying only relevant missing values.
More Flexible Outlier Detection

Your original code had a loop for boxplots but lacked modularity.
The AI-generated version uses detect_outliers(df, columns), allowing easy expansion by just adding more column names.
Enhanced Categorical Data Analysis

The AI-generated version sorts categorical value counts before plotting, improving visualization clarity.
Better Code Structure

The AI-generated code applies display() to tables for a neater output in Jupyter Notebooks.
Functions make it easier to add or modify analyses.
"""

#Optimized Analysis

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Load dataset
df = pd.read_csv('/content/drive/My Drive/Coursework/AB_NYC_2019.csv')

# Display dataset info
def summarize_data(df):
    print("Dataset Information:")
    print(df.info(), "\n")

    print("First few rows:")
    display(df.head())

    print("Summary Statistics:")
    display(df.describe())

    print("Unique values per column:")
    print(df.nunique())

summarize_data(df)

# Handle missing values
def check_missing_values(df):
    missing_values = df.isnull().sum()
    missing_percent = (missing_values / len(df)) * 100
    missing_percent = missing_percent[missing_percent > 0].sort_values(ascending=False)

    print("Missing Values:")
    print(missing_values[missing_values > 0])
    print("\nPercentage of Missing Values:")
    print(missing_percent)

check_missing_values(df)

# Detect Outliers using Boxplots
def detect_outliers(df, columns):
    for col in columns:
        plt.figure(figsize=(8, 4))
        sns.boxplot(x=df[col])
        plt.title(f'Boxplot of {col}')
        plt.show()

outlier_columns = ['price', 'minimum_nights', 'calculated_host_listings_count']
detect_outliers(df, outlier_columns)

# Correlation Analysis
def plot_correlations(df):
    correlation_matrix = df.corr(numeric_only=True)
    plt.figure(figsize=(10, 6))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
    plt.title("Correlation Heatmap")
    plt.show()

plot_correlations(df)

# Categorical Data Analysis
def analyze_categorical(df, categorical_columns):
    for col in categorical_columns:
        print(f"Value Counts for {col}:")
        print(df[col].value_counts(), "\n")

        plt.figure(figsize=(8, 4))
        sns.countplot(x=col, data=df, order=df[col].value_counts().index)
        plt.title(f'Distribution of {col}')
        plt.xticks(rotation=45)
        plt.show()

categorical_columns = ['neighbourhood_group', 'neighbourhood', 'room_type']
analyze_categorical(df, categorical_columns)

# Feature Engineering
# Encode categorical variables
le = LabelEncoder()
df['neighbourhood_group'] = le.fit_transform(df['neighbourhood_group'])
df['room_type'] = le.fit_transform(df['room_type'])

# Fill missing values with median
df.fillna(df.median(numeric_only=True), inplace=True)

# Define features and target variable
X = df[['neighbourhood_group', 'room_type', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']]
y = df['price']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale numerical features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train Model (Random Forest Regressor)
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluate Model
def evaluate_model(y_test, y_pred):
    print("Mean Absolute Error:", mean_absolute_error(y_test, y_pred))
    print("Mean Squared Error:", mean_squared_error(y_test, y_pred))
    print("Root Mean Squared Error:", mean_squared_error(y_test, y_pred, squared=False))
    print("R-squared Score:", r2_score(y_test, y_pred))

evaluate_model(y_test, y_pred)